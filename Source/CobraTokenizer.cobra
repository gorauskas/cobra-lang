class CobraTokenizer
	inherits Tokenizer

	test
		# There are plenty of external Cobra source code tests that will exercise the lexer.
		# But here are a few basic tests to make sure the tokenizer has some viability.
		t = CobraTokenizer()
		# TODO:
		t.startSource('foo bar')
		tokens = t.allTokens()
		Tokenizer.checkTokens(tokens, 'ID ID EOL')

		t.restart()
		t.startSource('class Foo\n\t\tdef foo()\n\t\t\treturn 1')
#		t.startSource(
#'''class Foo
#	def foo()
#		return 1
#''')
		tokens = t.allTokens()
		Tokenizer.checkTokens(tokens, 'CLASS ID EOL INDENT DEF OPEN_CALL RPAREN EOL INDENT RETURN INTEGER_LIT EOL DEDENT DEDENT')

		t.restart()
		t.startSource('class Foo\n\tpass\n\nclass Bar\n\tpass')
#		t.startSource(
#	'''class Foo
#		pass
#
#	class Bar
#		pass
#	''')
		tokens = t.allTokens()
		Tokenizer.checkTokens(tokens, 'CLASS ID EOL INDENT PASS EOL EOL DEDENT CLASS ID EOL INDENT PASS EOL DEDENT')

		t.restart()
		t.startSource('class Foo\n\tdef foo()\n\t\treturn 1\n\nclass Bar\n\tpass\n')
#	t.startSource(
#'''class Foo
#	def foo()
#		return 1
#
#class Bar
#	pass
#''')
		tokens = t.allTokens()
		Tokenizer.checkTokens(tokens, 'CLASS ID EOL INDENT DEF OPEN_CALL RPAREN EOL INDENT RETURN INTEGER_LIT EOL EOL DEDENT DEDENT CLASS ID EOL INDENT PASS EOL DEDENT')

	var _indentCount as int
	var _substLBracketCount as int
	var _inSubstStringSingle = false
	var _inSubstStringDouble = false
	var _inDocString = false

	def init
		base.init()

	def init(verbosity as int)
		base.init()
		_verbosity = verbosity

	def addInfo(sb as StringBuilder) is override
		base.addInfo(sb)
		sb.append('_indentCount=[_indentCount], ')
		sb.append('_substLBracketCount=[_substLBracketCount], ')
		sb.append('_inSubstStringSingle=[_inSubstStringSingle], ')
		sb.append('_inDocString=[_inDocString]')

	# Note: The Tokenizer class handles it's input one line at a time,
	#       and retains the \n at the end of the line. This affects
	#       the regex design for the tokens below.

	get orderedTokenSpecs as List<of String> is override
		return [
			# whitespace
			r'BLANK_TABS_LINE_1		^[\t]+$',
			r'BLANK_TABS_LINE_2		^[\t]+\#.*$',
			r'INDENT_MIXED_1		^[\t]+[ ]+',
			r'INDENT_MIXED_2		^[ ]+[\t]+',
			r'INDENT_ALL_TABS		^[\t]+',
			r'NO_INDENT				^(?=[^\t\n#])',
			r'EOL					\n',
			r'SINGLE_LINE_COMMENT	\#.*',
			r'SPACE					[ \t]+',

			r'OPEN_GENERIC			[A-Za-z_][A-Za-z0-9_]*<of[ \n\r\t]',
			r'OPEN_IF				if\(',
			r'OPEN_CALL				[A-Za-z_][A-Za-z0-9_]*\(',

			r'FLOAT_LIT				\d[\d_]*\.\d+f',
			r'DECIMAL_LIT			\d[\d_]*\.\d+d?',
			r'INTEGER_LIT			\d[\d_]*',

			r'INT_SIZE				int[0-9]+',
			r'UINT_SIZE				uint[0-9]+',
			r'FLOAT_SIZE			float[0-9]+',

			r"CHAR_LIT_SINGLE		c'\\?.'",
			r'CHAR_LIT_DOUBLE		c"\\?."',

			# doc strings
			r'DOC_STRING_START		"""\n',

			# raw strings
			r"STRING_RAW_SINGLE		r'[^'\n]*'",
			r'STRING_RAW_DOUBLE		r"[^"\n]*"',

			# substituted strings
			r'RBRACKET_SPECIAL		]',
			r"STRING_START_SINGLE	'[^'\n\[]*\[",
			r"STRING_PART_SINGLE	\][^'\n\[]*\[",
			r"STRING_STOP_SINGLE	\][^'\n\[]*'",

			r'STRING_START_DOUBLE	"[^"\n\[]*\[',
			r'STRING_PART_DOUBLE	\][^"\n\[]*\[',
			r'STRING_STOP_DOUBLE	\][^"\n\[]*"',

			r'STRING_PART_FORMAT	:[^X"\n\[]*(?=])'.replace('X', "'"),

			# plain strings
			r"STRING_NOSUB_SINGLE	ns'[^'\n]*'",
			r'STRING_NOSUB_DOUBLE	ns"[^"\n]*"',

			r"STRING_SINGLE			'[^'\n]*'",
			r'STRING_DOUBLE			"[^"\n]*"',

			r'TOQ					to\?',
			r'ID					[A-Za-z_][A-Za-z0-9_]*',
		]

	get unorderedTokenSpecs as List<of String> is override
		return [
			r'SHARP_OPEN		\$sharp\(',
			r'DOT				\.',
			r'DOTDOT			\.\.',
			r'COLON				:',
			r'PLUS				\+',
			r'PLUSPLUS			\+\+',
			r'MINUSMINUS		\-\-',
			r'MINUS				-',
			r'STARSTAR			\*\*',
			r'STAR				\*',
			r'SLASHSLASH		//',
			r'SLASH				/',
			r'PERCENT			%',
			r'ASSIGN			=',
			r'LPAREN			\(',
			r'RPAREN			\)',
			r'LBRACKET			\[',
			r'RBRACKET			\]',
			r'LCURLY			\{',
			r'RCURLY			\}',
			r'SEMI				;',
			r'COMMA				,',
			r'DOUBLE_LT			<<',
			r'DOUBLE_GT			>>',
			r'DICT_OPEN			{',
			r'DICT_CLOSE		}',
			r'QUESTION			\?',
			r'BANG				\!',

			r'EQ				==',
			r'NE				<>',
			r'LT				<',
			r'GT				>',
			r'LE				<=',
			r'GE				>=',

			r'PLUS_EQUALS		\+=',
			r'MINUS_EQUALS		\-=',
			r'STAR_EQUALS		\*=',
			r'SLASH_EQUALS		\/=',
			r'PERCENT_EQUALS	%=',
			r'QUESTION_EQUALS	\?=',
			r'BANG_EQUALS		\!=',
		]

	get keywords as String is override
		# CC: this should be multiline string with comments. See CobraTokenizer.py
		return 'STOPSTOP use import namespace enum class inherits implements interface struct where must be callable cue def as get set pro var from body test shared virtual override ref vari require ensure old this base is assert branch on expect if else using while post for down to step break continue try catch success finally throw except event listen ignore raise and or not implies every all to to\\? pass print stop return in bool char int decimal float of passthrough true false nil dynamic same instance'

	def _reset is override
		base._reset()
		_indentCount = 0
		_substLBracketCount = 0

	def afterStart is override
		base.afterStart()
		# CC:
		# _tokenDefsByWhich['STRING_PART_SINGLE'].isActiveCall = lambda tokenizer: tokenizer.inSubstStringSingle
		# _tokenDefsByWhich['STRING_STOP_SINGLE'].isActiveCall = lambda tokenizer: tokenizer.inSubstStringSingle
		for which as String in 'RBRACKET_SPECIAL STRING_PART_SINGLE STRING_STOP_SINGLE STRING_PART_DOUBLE STRING_STOP_DOUBLE STRING_PART_FORMAT'.split(c' ')  # CC: remove "as String"
			_tokenDefsByWhich[which].isActive = false

	def isActiveCall(tok as TokenDef) as bool is override
		if tok.which=='STRING_PART_SINGLE' or tok.which=='STRING_STOP_SINGLE'
			return _inSubstStringSingle
		return true

	get _nextToken as IToken? is override
		# overridden to deliver the final DEDENTS to close out indentation
		tok = base._nextToken
		if tok is nil
			while _indentCount > 0
				t = Token(.lastToken.fileName, .lastToken.lineNum, 1, .lastToken.charNum, 'DEDENT', '', '')
				_tokenQueue.enqueue(t)
				_indentCount -= 1
			if _tokenQueue
				return _nextToken
			else
				return nil
		else
			return tok

	def onBLANK_TABS_LINE_1(tok as IToken) as IToken?
		# Eat these.
		# Don't muck with perceived indentation level as
		# these kinds of lines are irrelevant.
		#print '<> onBLANK_TABS_LINE_1'
		return nil

	def onBLANK_TABS_LINE_2(tok as IToken) as IToken?
		#print '<> onBLANK_TABS_LINE_2'
		return nil

	def onINDENT_MIXED_1(tok as IToken) as IToken?
		sb = StringBuilder()
		for c in tok.text
			if c==c' '
				sb.append(r'[SPACE]')
			else if c==c'\t'
				sb.append(r'[TAB]')
			else
				sb.append(c)
		_error('Cannot mix tabs and spaces in indentation. [sb]...')
		return nil  # make compiler happy.  CC?: new "never" statement. meaning execution should never get here (and therefore no return is needed). Throws NeverException.

	def onINDENT_MIXED_2(tok as IToken) as IToken?
		return .onINDENT_MIXED_1(tok)

	def onINDENT_ALL_TABS(tok as IToken) as IToken?
		numTabs = _countChars(tok.text, c'\t')  # CC: tok.text.count(c'\t')
		return _processNumTabs(numTabs)

	def onNO_INDENT(tok as IToken) as IToken?
		require tok.text==''
		_curTokenDef.ignoreCount = 1
		t = _processNumTabs(0)
		return t

	def _processNumTabs(numTabs as int) as IToken?
		firstTok as IToken? = nil  # CC: should be able to leave out type by inferring type from below
		lastTok as IToken? = nil  # CC: same as above the line
		while numTabs>_indentCount
			_indentCount += 1
			newt = Token(_fileName, _lineNum, 1, _charNum, 'INDENT', '', '')
			if lastTok
				lastTok.nextToken = newt
				lastTok = newt
			else
				firstTok = lastTok = newt
			numTabs -= 1
		if firstTok
			return firstTok
		while numTabs<_indentCount
			_indentCount -= 1
			newt = Token(_fileName, _lineNum, 1, _charNum, 'DEDENT', '', '')
			if lastTok
				lastTok.nextToken = newt
				lastTok = newt
			else
				firstTok = lastTok = newt
		return firstTok

	def onEOL(tok as IToken) as IToken?
		return tok

	def onSINGLE_LINE_COMMENT(tok as IToken) as IToken?
		# eat these
		return nil

	def onSPACE(tok as IToken) as IToken?
		# eat these
		return nil

	# TODO: what the hell is CON for?
	def onCON(tok as IToken) as IToken?
		tok1 = tok.copy
		tok1.text = 'con'
		tok1.value = 'con'
		tok2 = tok.copy
		tok2.which = 'LPAREN'
		tok2.value = tok2.text = '('
		tok1.nextToken = tok2
		return tok1

	def onOPEN_CALL(tok as IToken) as IToken?
		tok.value = tok.text.substring(0, tok.text.length-1)  # CC: might be nice to say: ... = tok.text[:-1]
		return tok

	def onOPEN_GENERIC(tok as IToken) as IToken?
		require tok.text.trim().endsWith('<of')
		s = tok.text.trim()
		tok.value = s.substring(0, s.length-3)  # CC: might be nice to say: ... = s[:-3]
		return tok

	def onID(tok as IToken) as IToken?
		return .keywordOrWhich(tok, 'ID')

	def onFLOAT_LIT(tok as IToken) as IToken?
		try
			s = tok.text.replace('_', '')
			if s.endsWith('f')
				s = s.substring(0, s.length-1)  # CC: s[:-1]
			tok.value = float.parse(s)
		catch fe as FormatException
			assert false, 'not expecting to get here given regex'
		catch oe as OverflowException
			assert false, 'TODO'
		return tok

	def onDECIMAL_LIT(tok as IToken) as IToken?
		try
			s = tok.text.replace('_', '')
			if s.endsWith('d')
				s = s.substring(0, s.length-1)  # CC: [:-1]
			tok.value = decimal.parse(s)
		catch fe as FormatException
			assert false, 'not expecting to get here given regex'
		catch oe as OverflowException
			assert false, 'TODO'
		return tok

	def onINTEGER_LIT(tok as IToken) as IToken?
		try
			tok.value = int.parse(tok.text.replace('_', ''))
		catch fe as FormatException
			assert false, 'not expecting to get here given regex'
		catch oe as OverflowException
			assert false, 'TODO'
		return tok

	def onINT_SIZE(tok as IToken) as IToken?
		size = int.parse(tok.text.substring(3)) to int  # CC: [3:], CC: axe "to int"
		#try:
		if 1
			# TODO: tok.value = tintForSize[size]
			tok.value = size
		#except KeyError
		#	throw TokenizerError('Unsupported integer size: [size]')
		return tok

	def onUINT_SIZE(tok as IToken) as IToken?
		size = int.parse(tok.text.substring(4)) to int  # CC: [3:], CC: axe "to int"
		#try:
		if 1
			# TODO: tok.value = tuintForSize[size]
			tok.value = size
		#except KeyError
		#	throw TokenizerError('Unsupported integer size: [size]')
		return tok

	def onFLOAT_SIZE(tok as IToken) as IToken?
		size = int.parse(tok.text.substring(5)) to int  # CC: [5:], CC: axe "to int"
		tok.value = size
		return tok
		# TODO: finish this

	def onCHAR_LIT_SINGLE(tok as IToken) as IToken?
		return _onCharLit(tok)

	def onCHAR_LIT_DOUBLE(tok as IToken) as IToken?
		return _onCharLit(tok)

	def _onCharLit(tok as IToken) as IToken?
		require tok.text.startsWith('c')
		s = tok.text.substring(2, tok.text.length-3)  # CC: [2:-1]
		assert s.length==1 or s.length==2
		tok.value = s
		return tok


	##
	## String substitution handling
	##

	def tokValueForString(s as String) as String
		"""
		Utility class for onSTRING_START|PART|STOP_SINGLE|DOUBLE.
		"""
		require
			s
			s.length>=2  # CC: #s[0] in [c'"', c"'"] #s[s.length-1] in [c'"', c"'"]
		body
			s = s.substring(1, s.length-2)
			chars = StringBuilder()  # TODO: make an array of chars
			last = c'\0'
			next as char?
			for c in s
				next = nil
				if last==c'\\'
					branch c
						# TODO: any other codes?
						on c'n': next = c'\n'
						on c'r': next = c'\r'
						on c't': next = c'\t'
						on c'0': next = c'\0'
						on c'\\'
							chars.append(c'\\')
							# cannot have `last` being a backslash anymore--it's considered consumed now
							last = c'\0'
							continue
						else: next = c # TODO: should probably be error: Invalid escape sequence
				else if c<>'\\'
					next = c
				if next<>nil
					chars.append(next)
				last = c
			return chars.toString

	def onSTRING_START_SINGLE(tok as IToken) as IToken
		require not _inSubstStringSingle
		_inSubstStringSingle = true
		tok.value = .tokValueForString(tok.text)
		_tokenDefsByWhich['STRING_PART_SINGLE'].isActive = true
		_tokenDefsByWhich['STRING_STOP_SINGLE'].isActive = true
		_tokenDefsByWhich['STRING_PART_FORMAT'].isActive = true
		return tok

	def onSTRING_PART_SINGLE(tok as IToken) as IToken
		require _inSubstStringSingle
		tok.value = .tokValueForString(tok.text)
		return tok

	def onSTRING_STOP_SINGLE(tok as IToken) as IToken
		require _inSubstStringSingle
		_inSubstStringSingle = false
		tok.value = .tokValueForString(tok.text)
		_tokenDefsByWhich['STRING_PART_SINGLE'].isActive = false
		_tokenDefsByWhich['STRING_STOP_SINGLE'].isActive = false
		_tokenDefsByWhich['STRING_PART_FORMAT'].isActive = false
		return tok


	def onSTRING_START_DOUBLE(tok as IToken) as IToken
		require not _inSubstStringDouble
		_inSubstStringDouble = true
		tok.value = .tokValueForString(tok.text)
		_tokenDefsByWhich['STRING_PART_DOUBLE'].isActive = true
		_tokenDefsByWhich['STRING_STOP_DOUBLE'].isActive = true
		_tokenDefsByWhich['STRING_PART_FORMAT'].isActive = true
		return tok

	def onSTRING_PART_DOUBLE(tok as IToken) as IToken
		require _inSubstStringDouble
		tok.value = .tokValueForString(tok.text)
		return tok

	def onSTRING_STOP_DOUBLE(tok as IToken) as IToken
		require _inSubstStringDouble
		_inSubstStringDouble = false
		tok.value = .tokValueForString(tok.text)
		_tokenDefsByWhich['STRING_PART_DOUBLE'].isActive = false
		_tokenDefsByWhich['STRING_STOP_DOUBLE'].isActive = false
		_tokenDefsByWhich['STRING_PART_FORMAT'].isActive = false
		return tok


	def onLBRACKET(tok as IToken) as IToken
		if _inSubstStringSingle or _inSubstStringDouble
			_substLBracketCount += 1
			if _substLBracketCount==1
				_tokenDefsByWhich['RBRACKET_SPECIAL'].isActive = true
		return tok

	def onRBRACKET_SPECIAL(tok as IToken) as IToken
		require
			_inSubstStringSingle or _inSubstStringDouble
			_substLBracketCount
		body
			_substLBracketCount -= 1
			if _substLBracketCount==0
				_tokenDefsByWhich['RBRACKET_SPECIAL'].isActive = false
			tok.which = 'RBRACKET'  # tricky, tricky. the parser never sees an RBRACKET_SPECIAL
			return tok


	##
	## Doc Strings
	##

	def onDOC_STRING_START(tok as IToken) as IToken
		assert not _inDocString
		# narrow the tokenizer's token defs to a new shorter set
		# TODO: cache the tokens below
		.pushTokenDefs([
			TokenDef('DOC_STRING_STOP', '\t*"""\n'),
			TokenDef('DOC_STRING_BODY_TEXT', '.*\n'),
		])
		_inDocString = true
		#print '<> inDocString is now %r' % self.inDocString
		return tok

	def onDOC_STRING_STOP(tok as IToken) as IToken
		assert _inDocString
		_inDocString = false
		.popTokenDefs()
		return tok

	def onDOC_STRING_BODY_TEXT(tok as IToken) as IToken
		assert _inDocString, tok
		return tok


	##
	## Simple string literals
	##

	def onSTRING_RAW_SINGLE(tok as IToken) as IToken
		require tok.text.startsWith('r')
		tok.value = tok.text.substring(2, tok.text.length-3)
		tok.which = 'STRING_SINGLE'
		return tok

	def onSTRING_RAW_DOUBLE(tok as IToken) as IToken
		require tok.text.startsWith('r')
		tok.value = tok.text.substring(2, tok.text.length-3)
		tok.which = 'STRING_DOUBLE'
		return tok

	def onSTRING_NOSUB_SINGLE(tok as IToken) as IToken
		require tok.text.startsWith('ns')
		tok.value = .tokValueForString(tok.text.substring(2))
		tok.which = 'STRING_SINGLE'
		return tok

	def onSTRING_NOSUB_DOUBLE(tok as IToken) as IToken
		require tok.text.startsWith('ns')
		tok.value = .tokValueForString(tok.text.substring(2))
		tok.which = 'STRING_DOUBLE'
		return tok

	def onSTRING_SINGLE(tok as IToken) as IToken
		tok.value = .tokValueForString(tok.text)
		return tok

	def onSTRING_DOUBLE(tok as IToken) as IToken
		tok.value = .tokValueForString(tok.text)
		return tok


	##
	## Deprecated
	##

	def onCUE(tok as IToken) as IToken
		assert false, tok  # wow, this doesn't even work
		tok.which = 'DEF'
		return tok
